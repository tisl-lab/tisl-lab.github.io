<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.306">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ulrich A√Øvodji">
<meta name="dcterms.date" content="2019-04-24">
<meta name="description" content="Summary of a paper: Fairwashing ‚Äì the risk of rationalization">

<title>TISL - Fairwashing in machine learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../css/styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">TISL</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html" rel="" target=""><i class="bi bi-house" role="img">
</i> 
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../publications.html" rel="" target=""><i class="bi bi-book" role="img">
</i> 
 <span class="menu-text">Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html" rel="" target=""><i class="bi bi-pen" role="img">
</i> 
 <span class="menu-text">Blog</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#prerequisites" id="toc-prerequisites" class="nav-link active" data-scroll-target="#prerequisites">Prerequisites üö≤</a>
  <ul class="collapse">
  <li><a href="#fairness" id="toc-fairness" class="nav-link" data-scroll-target="#fairness">Fairness ‚öñÔ∏è</a></li>
  <li><a href="#black-box-explanation" id="toc-black-box-explanation" class="nav-link" data-scroll-target="#black-box-explanation">Black-box explanation</a></li>
  <li><a href="#rule-lists" id="toc-rule-lists" class="nav-link" data-scroll-target="#rule-lists">Rule lists üóíÔ∏è</a></li>
  <li><a href="#corels" id="toc-corels" class="nav-link" data-scroll-target="#corels">CORELS</a></li>
  <li><a href="#model-enumeration" id="toc-model-enumeration" class="nav-link" data-scroll-target="#model-enumeration">Model enumeration</a></li>
  </ul></li>
  <li><a href="#rationalization" id="toc-rationalization" class="nav-link" data-scroll-target="#rationalization">Rationalization</a>
  <ul class="collapse">
  <li><a href="#problem-formulation" id="toc-problem-formulation" class="nav-link" data-scroll-target="#problem-formulation">Problem formulation</a></li>
  <li><a href="#laundryml" id="toc-laundryml" class="nav-link" data-scroll-target="#laundryml">LaundryML</a></li>
  </ul></li>
  <li><a href="#experiments" id="toc-experiments" class="nav-link" data-scroll-target="#experiments">Experiments</a>
  <ul class="collapse">
  <li><a href="#results-for-model-rationalization" id="toc-results-for-model-rationalization" class="nav-link" data-scroll-target="#results-for-model-rationalization">Results for model rationalization</a></li>
  <li><a href="#results-for-outcome-rationalization" id="toc-results-for-outcome-rationalization" class="nav-link" data-scroll-target="#results-for-outcome-rationalization">Results for outcome rationalization</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Fairwashing in machine learning</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Explainability</div>
    <div class="quarto-category">Fairness</div>
    <div class="quarto-category">Fairwashing</div>
  </div>
  </div>

<div>
  <div class="description">
    <p>Summary of a paper: Fairwashing ‚Äì the risk of rationalization</p>
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p><a href="https://aivodji.github.io/">Ulrich A√Øvodji</a> </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 24, 2019</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>Machine learning is now used in every aspect of our life, from entertainment to high stakes decision-making processes such as credit scoring, medical diagnosis, or predictive justice. The potential risk of incorrect decisions has raised the public demand for an explanation of the decisions of machine learning models. In addition to this public demand, all across the world, several communities and government initiatives are emerging, asking for more transparency in machine learning models‚Äô decisions, and the development of an ethically-aligned AI. As an example, in Europe, the new General Data Protection Regulation has a provision requiring explanations for the decisions of machine learning models that have a significant impact on individuals <span class="citation" data-cites="goodman2016european">(<a href="#ref-goodman2016european" role="doc-biblioref">Goodman and Flaxman 2017</a>)</span>.</p>
<p>We believe that because of this particular combination of regulations and public demand for ethically-aligned development of AI, a dishonest machine learning models‚Äô producers may be tempted to perform fairwashing. We define fairwashing as promoting the false perception that a machine learning model complies with a given ethical requirement while it might not be so. The risk of fairwashing is all the more possible because the right to explanation as defined in current regulations does not give precise directives on what it means to provide a ‚Äò‚Äôvalid explanation‚Äô‚Äô <span class="citation" data-cites="wachter2017right edwards2017slave">(<a href="#ref-wachter2017right" role="doc-biblioref">Wachter, Mittelstadt, and Floridi 2017</a>; <a href="#ref-edwards2017slave" role="doc-biblioref">Edwards and Veale 2017</a>)</span>, leaving a legal loophole that can be exploited by a dishonest model‚Äôs producer to cover up a possible misconducts of its black-box model by providing misleading explanations.</p>
<p>To demonstrate this risk, we consider two variations of the black-box explanation problem and show that one can forge misleading explanations that comply with a given ethical requirement. In particular, we use fairness as the ethical requirement and demonstrate that given a black-box model that is unfair, one can systematically deduce rule lists with high fidelity to the black-box model while being considerably less unfair at the same time.</p>
<section id="prerequisites" class="level1">
<h1>Prerequisites üö≤</h1>
<section id="fairness" class="level2">
<h2 class="anchored" data-anchor-id="fairness">Fairness ‚öñÔ∏è</h2>
<p>There are several definitions for fairness in machine learning <span class="citation" data-cites="verma2018fairness">(<a href="#ref-verma2018fairness" role="doc-biblioref">Verma and Rubin 2018</a>)</span>. Central to all these definitions is the notion of the sensitive attribute <span class="math inline">\(s\)</span> for which non-discrimination should be established. An example of such attribute can be the gender, ethnicity, or religion. For this work, we use demographic parity <span class="citation" data-cites="calders2009building">(<a href="#ref-calders2009building" role="doc-biblioref">Calders, Kamiran, and Pechenizkiy 2009</a>)</span> as fairness metric. Demographic parity requires the prediction <span class="math inline">\(\hat{y}\)</span> of the black-box model to be independent of the sensitive attribute <span class="math inline">\(s\)</span>, that is, <span class="math inline">\(P(\hat{y}=1 | s= 1) = P(\hat{y}=1 | s= 0)\)</span>. We therefore define unfairness as <span class="math inline">\(\mathsf{unfairness}=|P(\hat{y}=1 | s= 1) - P(\hat{y}=1 | s= 0)|\)</span>.</p>
</section>
<section id="black-box-explanation" class="level2">
<h2 class="anchored" data-anchor-id="black-box-explanation">Black-box explanation</h2>
<p>Black-box explanation is the problem of explaining how a machine learning model ‚Äì whose internal logic is hidden to the auditor and generally complex ‚Äì produces its outcomes. An explanation can be viewed as an interface between humans and a decision process that is both an accurate proxy of the decision process and understandable by humans <span class="citation" data-cites="guidotti2018survey">(<a href="#ref-guidotti2018survey" role="doc-biblioref">Guidotti et al. 2018</a>)</span>. Figure @ref(fig:pipeline) shows an example of a black-box explanation pipeline. First, a black-box model <span class="math inline">\(M_b\)</span> is obtained using a particular learning algorithm (e.g., random forest, SVM, neural network‚Ä¶) on a particular dataset <span class="math inline">\(D_{train}\)</span>. Then, the black-box model is used to label another dataset <span class="math inline">\(D_{labeled} \neq D_{train}\)</span>. Finally, an explanation algorithm takes as input <span class="math inline">\(D_{labeled}\)</span> and produces an interpretable model <span class="math inline">\(M_i\)</span>. Examples of interfaces accepted in the literature as interpretable models include linear models, decision trees, rule lists, and rule sets. In this work, we use rule lists as explanation models. We consider two variations of the black-box explanation problem, namely model explanation and outcome explanation. The former consists in providing an explanation model that explains all the decisions of the black-box model, while the latter consists of producing an explanation model that explains a particular decision of the black-box model. To assess the performance of an explanation algorithm, we rely on the notion of fidelity <span class="citation" data-cites="craven1996extracting">(<a href="#ref-craven1996extracting" role="doc-biblioref">Craven and Shavlik 1996</a>)</span>, which is the accuracy of the explanation model <span class="math inline">\(M_i\)</span> relative to the black-box model <span class="math inline">\(M_b\)</span> on some instances <span class="math inline">\(X\)</span>. Simply put, <span class="math inline">\(\mathsf{fidelity} = \frac{1}{|X|} \sum_{x \in X} \mathbb{I}(M_i(x)=M_b(x))\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="explanation.png" class="img-fluid figure-img" width="340"></p>
<p></p><figcaption class="figure-caption">Black-box explanation pipeline</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="rule-lists" class="level2">
<h2 class="anchored" data-anchor-id="rule-lists">Rule lists üóíÔ∏è</h2>
<p>A rule list <span class="citation" data-cites="rivest1987learning">(<a href="#ref-rivest1987learning" role="doc-biblioref">Rivest 1987</a>)</span> <span class="math inline">\(r= (d_p, \delta_p, q_0, K)\)</span> of length <span class="math inline">\(K \geq 0\)</span> is a <span class="math inline">\((K+1)-\)</span>tuple consisting of <span class="math inline">\(K\)</span> distinct association rules <span class="math inline">\(p_k \to q_k\)</span>, in which <span class="math inline">\(p_k \in d_p\)</span> is the antecedent of the association rule and <span class="math inline">\(q_k \in \delta_p\)</span> its corresponding consequent, followed by a default prediction <span class="math inline">\(q_0\)</span>. The rule list below predicts whether a person is likely to make at least 50k per year.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode haskell code-with-copy"><code class="sourceCode haskell"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a> <span class="kw">if</span> [capitalGain<span class="op">:&gt;=</span><span class="dv">5119</span>] <span class="kw">then</span> [high]</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="kw">else</span> <span class="kw">if</span> [maritalStatus<span class="op">:</span>single] <span class="kw">then</span> [low]</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">else</span> <span class="kw">if</span> [capitalLoss<span class="op">:</span><span class="dv">1882</span><span class="op">-</span><span class="dv">1978</span>] <span class="kw">then</span> [high]</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="kw">else</span> <span class="kw">if</span> [workclass<span class="op">:</span>private <span class="op">&amp;&amp;</span> education<span class="op">:</span>bachelors] <span class="kw">then</span> [high]</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="kw">else</span> <span class="kw">if</span> [education<span class="op">:</span>masters_doctorate] <span class="kw">then</span> [high]</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="kw">else</span> [low]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To make a prediction using a rule list, the rules are applied sequentially until one rule applies, in which case the associated outcome is reported. If none of the rules applies, then the default prediction is reported.</p>
</section>
<section id="corels" class="level2">
<h2 class="anchored" data-anchor-id="corels">CORELS</h2>
<p>CORELS <span class="citation" data-cites="angelino2018learning">(<a href="#ref-angelino2018learning" role="doc-biblioref">Angelino et al. 2018</a>)</span> is a supervised machine learning algorithm which takes as input a feature vector <span class="math inline">\(X\)</span> and it associated label vector <span class="math inline">\(Y\)</span>, all assumed to be binary, and finds the rule list <span class="math inline">\(r\)</span> that minimize the regularized empirical risk: <span class="math inline">\(\mathsf{misc(r, X, Y)} + \lambda K\)</span>, where <span class="math inline">\(\mathsf{misc(r, X, Y)}\)</span> is the misclassification error of the rule list and <span class="math inline">\(\lambda \geq 0\)</span> is a regularization parameter used to penalize longer rule lists. It represents the search space of the rule lists as a trie and uses an efficient branch-and-bound algorithm to prune it.</p>
</section>
<section id="model-enumeration" class="level2">
<h2 class="anchored" data-anchor-id="model-enumeration">Model enumeration</h2>
<p>In this work, we use a rule list enumeration technique introduced in <span class="citation" data-cites="hara2018approximate">(<a href="#ref-hara2018approximate" role="doc-biblioref">Hara and Ishihata 2018</a>)</span>. This algorithm takes as input the set <span class="math inline">\(T\)</span> of all binary predictors, and enumerate rule list models in the descending order of their objective function. It maintains a heap <span class="math inline">\(\mathcal{H}\)</span>, whose priority is the objective function value of the rule lists and a list <span class="math inline">\(\mathcal{M}\)</span> for the enumerated models. First, it starts by computing the optimal rule <span class="math inline">\(m=\mathsf{CORELS}(T) = (d_p, \delta_p, q_0, K)\)</span> for <span class="math inline">\(T\)</span> using the CORELS algorithm. Then, it inserts the tuple <span class="math inline">\((m, T, \emptyset)\)</span> into the heap. Finally, the algorithm repeats the following three steps until the stopping criterion is met: * Extract the tuple <span class="math inline">\((m, S, F)\)</span> with the maximum priority value from <span class="math inline">\(\mathcal{H}\)</span>. * Output <span class="math inline">\(m\)</span> as the <span class="math inline">\(i-\)</span>th model if <span class="math inline">\(m \notin \mathcal{M}\)</span>. * Branch the search space: compute and insert <span class="math inline">\(m' =\mathsf{CORELS}(S \setminus \{t_j\})\)</span> into <span class="math inline">\(\mathcal{H}\)</span> for all <span class="math inline">\(t_j \in \delta_p\)</span>.</p>
</section>
</section>
<section id="rationalization" class="level1">
<h1>Rationalization</h1>
<section id="problem-formulation" class="level2">
<h2 class="anchored" data-anchor-id="problem-formulation">Problem formulation</h2>
<p>We define rationalization as the problem of finding an interpretable surrogate model <span class="math inline">\(M_i\)</span> approximating a black-box model <span class="math inline">\(M_b\)</span>, such that <span class="math inline">\(M_i\)</span> is fairer than <span class="math inline">\(M_b\)</span>. In particular, we distinguish model rationalization and outcome rationalization.</p>
<p>Given a black-box model <span class="math inline">\(M_b\)</span>, a set of instances <span class="math inline">\(X\)</span> and a sensitive attribute <span class="math inline">\(s\)</span>, the model rationalization problem consists in finding an intrepretable global model <span class="math inline">\(M_i^g\)</span> such that <span class="math inline">\(\epsilon(M_i^g, X, s) &gt; \epsilon(M_b, X, s)\)</span>, for some fairness evaluation metric <span class="math inline">\(\epsilon(\cdot, \cdot, \cdot)\)</span>. The outcome rationalization problem is similar to the model rationalaization problem, but consider only a single data point, and provide an explanation for this particular data point and its neighborhood. More precisely, given a black-box model <span class="math inline">\(M_b\)</span>, an instance <span class="math inline">\(x\)</span>, a neighborhood <span class="math inline">\(\mathcal{V}(x)\)</span> of <span class="math inline">\(x\)</span>, and a sensitive attribute <span class="math inline">\(s\)</span>, the outcome rationalization problem consists in finding an intrepretable local model <span class="math inline">\(M_i^l\)</span> such that <span class="math inline">\(\epsilon(M_i^l, \mathcal{V}(x), s) &gt; \epsilon(M_b, \mathcal{V}(x), s)\)</span>, for some fairness evaluation metric <span class="math inline">\(\epsilon(\cdot, \cdot, \cdot)\)</span>.</p>
</section>
<section id="laundryml" class="level2">
<h2 class="anchored" data-anchor-id="laundryml">LaundryML</h2>
<p>We propose LaundryML, an algorithm to solve the rationalization problem efficiently. First, LaundryML uses a modified version of CORELS in which we define the new objective function as <span class="math inline">\(\mathsf{(1-\beta) misc(\cdot)} + \mathsf{\beta unfairness(\cdot)}+ \lambda K\)</span>, where <span class="math inline">\(\mathsf{misc(\cdot)}\)</span> is the misclassification error of the rule list, <span class="math inline">\(\mathsf{unfairness(\cdot)}\)</span> returns the unfairness of the rule list, <span class="math inline">\(\lambda \geq 0\)</span> is a regularization parameter used to penalize longer rule lists, and <span class="math inline">\(\beta \geq 0\)</span> is the weight of the unfairness regularization. Then, it uses the customized CORELS algorithm to enumerated rule list models. Finally, it uses a threshold for the unfairness to select the models that can be used for rationalization.</p>
<p>To solve the model rationalization problem for a set <span class="math inline">\(X\)</span> of instances, LaundryML will take as input the tuple <span class="math inline">\(T=\{X, y\}\)</span> formed the instances <span class="math inline">\(X\)</span> and the predictions <span class="math inline">\(y\)</span> of the black-box model on <span class="math inline">\(X\)</span>. To solve the outcome rationalization for a particular instance <span class="math inline">\(x\)</span>, the inputs of LaundryML will be the instances <span class="math inline">\(T_x = \mathsf{neigh(x, T)}\)</span>, some neighborhood searching algorithm <span class="math inline">\(\mathsf{neigh(\cdot)}\)</span>.</p>
</section>
</section>
<section id="experiments" class="level1">
<h1>Experiments</h1>
<p>We train a Random Forest model on Adult Income dataset to mimic the black-box model. Our black-box model achieves respectively an accuracy of <span class="math inline">\(84.31\%\)</span>, a precision of <span class="math inline">\(79.78\%\)</span>, and an unfairness of <span class="math inline">\(0.13\)</span> on a set of instances we use to mimic a suing group (that accuses the black-box model on being unfair).</p>
<section id="results-for-model-rationalization" class="level2">
<h2 class="anchored" data-anchor-id="results-for-model-rationalization">Results for model rationalization</h2>
<p>We perform fairwashaing through model rationalization by explaining the decisions of the black-box model with an interpretable model (here a rule list) that is less unfair than the black-box while having a fidelity greater than <span class="math inline">\(90\%\)</span>. For the model rationalization experiment, we set <span class="math inline">\(\lambda=0.005\)</span>, and <span class="math inline">\(\beta =\{0, 0.1, 0.2, 0.5, 0.7, 0.9\}\)</span>, and we enumerate 50 models.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fairwashing_global.png" class="img-fluid figure-img" width="881"></p>
<p></p><figcaption class="figure-caption">CDFs of the unfairness (left) and the fidelity (right) of rationalized explanation models produced by LaundryML on the suing group of Adult Income. Results are for the demographic parity metric and the random forest black-box model. The vertical line on the left figure represents the unfairness of the black-box model. The CDFs on the right figure are the CDFs of the fidelity of explanation models that are fairer than the black box model.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The results in Figure @ref(fig:global) show that one can obtain fairer explanation models that have good fidelity. In particular, we observe that as <span class="math inline">\(\beta\)</span> increases, both the unfairness and the fidelity of the enumerated models decrease. Overall, the best model we obtain has a fidelity of <span class="math inline">\(0.908\)</span> and an unfairness of <span class="math inline">\(0.058\)</span>. In Figure @ref(fig:audit), we compare the selected rationalization model to the black-box model in term of relative feature dependence ranking. The observations show that the sensitive attribute is ranked <span class="math inline">\(2\)</span>nd (respectively <span class="math inline">\(28\)</span>th) with the black-box model (respectively the rationalization model).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="audit.png" class="img-fluid figure-img" width="1406"></p>
<p></p><figcaption class="figure-caption">Relative feature dependence ranking obtained using FairML to audit models trained on the Adult Income dataset. Green indicates that the feature highly contributes to a high salary rating on Adult. Features that characterize the majority groups are highlighted in yellow. Black-box model (left) vs.&nbsp;LaundryML model (middle) and its description (right).</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="results-for-outcome-rationalization" class="level2">
<h2 class="anchored" data-anchor-id="results-for-outcome-rationalization">Results for outcome rationalization</h2>
<p>We perform fairwhasing trough outcome rationalization by explaining each rejected female subject with a rule list whose unfairness (as measured on the neighbourhood of the subject) is lower than that of the black-box model. For the outcome rationalization experiment, we set <span class="math inline">\(\lambda=0.005\)</span>, and <span class="math inline">\(\beta =\{0.1, 0.3, 0.5, 0.7, 0.9\}\)</span>, and for each rejected female subjects, we enumerate 50 models to perform outcome rationalization.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="local_adult.png" class="img-fluid figure-img" width="675"></p>
<p></p><figcaption class="figure-caption">CDFs of the unfairness of the best model found by LaundryML per user on Adult Income</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Results in Figure @ref(fig:local) show that as the fairness regulation parameter <span class="math inline">\(\beta\)</span> increases, the unfairness of the explanation model decreases. In particular, with <span class="math inline">\(\beta=0.9\)</span>, a completely fair model (unfairness = <span class="math inline">\(0.0\)</span>) was found to explain the outcome for each rejected female subject in Adult Income.</p>
<p>Results on ProPublica Recidivism dataset, as well as experiments using other black-box models and fairness metrics, are discussed in our paper <span class="citation" data-cites="aivodji2019fairwashing">(<a href="#ref-aivodji2019fairwashing" role="doc-biblioref">A√Øvodji et al. 2019</a>)</span>.</p>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>This work introduces the risk of fairwashing associated with black-box explanation in machine learning. We hope our work will raise the awareness of the machine learning community and inspire future research towards the ethical issues raised by the possibility of rationalizing.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-aivodji2019fairwashing" class="csl-entry" role="listitem">
A√Øvodji, Ulrich, Hiromi Arai, Olivier Fortineau, S√©bastien Gambs, Satoshi Hara, and Alain Tapp. 2019. <span>‚ÄúFairwashing: The Risk of Rationalization.‚Äù</span> In <em>International Conference on Machine Learning</em>, 161‚Äì70. PMLR.
</div>
<div id="ref-angelino2018learning" class="csl-entry" role="listitem">
Angelino, Elaine, Nicholas Larus-Stone, Daniel Alabi, Margo Seltzer, and Cynthia Rudin. 2018. <span>‚ÄúLearning Certifiably Optimal Rule Lists for Categorical Data.‚Äù</span> <em>Journal of Machine Learning Research</em> 18 (234): 1‚Äì78.
</div>
<div id="ref-calders2009building" class="csl-entry" role="listitem">
Calders, Toon, Faisal Kamiran, and Mykola Pechenizkiy. 2009. <span>‚ÄúBuilding Classifiers with Independency Constraints.‚Äù</span> In <em>Proceedings of the IEEE International Conference on Data Mining Workshops (ICDMW‚Äô09)</em>, 13‚Äì18. IEEE.
</div>
<div id="ref-craven1996extracting" class="csl-entry" role="listitem">
Craven, Mark, and Jude W Shavlik. 1996. <span>‚ÄúExtracting Tree-Structured Representations of Trained Networks.‚Äù</span> In <em>Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS‚Äô96)</em>, 24‚Äì30.
</div>
<div id="ref-edwards2017slave" class="csl-entry" role="listitem">
Edwards, Lilian, and Michael Veale. 2017. <span>‚ÄúSlave to the Algorithm: Why a Right to an Explanation Is Probably Not the Remedy You Are Looking For.‚Äù</span> <em>Duke L. &amp; Tech. Rev.</em> 16: 18.
</div>
<div id="ref-goodman2016european" class="csl-entry" role="listitem">
Goodman, B, and SR Flaxman. 2017. <span>‚ÄúEuropean Union Regulations on Algorithmic Decision-Making and a <span>‚ÄòRight to Explanation‚Äô</span>.‚Äù</span> <em>AI Magazine</em> 38 (3): 50‚Äì57.
</div>
<div id="ref-guidotti2018survey" class="csl-entry" role="listitem">
Guidotti, Riccardo, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi. 2018. <span>‚ÄúA Survey of Methods for Explaining Black Box Models.‚Äù</span> <em>ACM Computing Surveys (CSUR)</em> 51 (5): 93.
</div>
<div id="ref-hara2018approximate" class="csl-entry" role="listitem">
Hara, Satoshi, and Masakazu Ishihata. 2018. <span>‚ÄúApproximate and Exact Enumeration of Rule Models.‚Äù</span> In <em>Proceedings of the Thirty-Second <span>AAAI</span> Conference on Artificial Intelligence (AAAI‚Äô18)</em>, 3157‚Äì64.
</div>
<div id="ref-rivest1987learning" class="csl-entry" role="listitem">
Rivest, Ronald L. 1987. <span>‚ÄúLearning Decision Lists.‚Äù</span> <em>Machine Learning</em> 2 (3): 229‚Äì46.
</div>
<div id="ref-verma2018fairness" class="csl-entry" role="listitem">
Verma, Sahil, and Julia Rubin. 2018. <span>‚ÄúFairness Definitions Explained.‚Äù</span> In <em>2018 Ieee/Acm International Workshop on Software Fairness (Fairware)</em>, 1‚Äì7. IEEE.
</div>
<div id="ref-wachter2017right" class="csl-entry" role="listitem">
Wachter, Sandra, Brent Mittelstadt, and Luciano Floridi. 2017. <span>‚ÄúWhy a Right to Explanation of Automated Decision-Making Does Not Exist in the General Data Protection Regulation.‚Äù</span> <em>International Data Privacy Law</em> 7 (2): 76‚Äì99.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Made with <a href="https://quarto.org/">Quarto</a></div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/tisl-lab">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:ulrich.aivodji@etsmtl.ca">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>



</body></html>