[
  {
    "objectID": "members.html",
    "href": "members.html",
    "title": "Lab Members",
    "section": "",
    "text": "Dave-Harold Mbiazi-Njanda, MSc Student\n    ☘️ co-supervision with \n  Foutse Khomh\n  \n  Eliott Baltz, MSc Student\n  \n  Maryam Babaie, PhD Student\n    ☘️ co-supervision with \n  Sébastien Gambs\n  \n  Meghana Bhange, MSc Student\n    ☘️ co-supervision with \n  Jean-Marc Robert\n  \n  Patrik Kenfack, PhD Student\n    ☘️ co-supervision with \n  Samira Ebrahimi Kahou\n  \n  Ulrich Aïvodji, Faculty\n  \n\nNo matching items"
  },
  {
    "objectID": "coc.html",
    "href": "coc.html",
    "title": "Lab Code of conduct",
    "section": "",
    "text": "TISL is dedicated to providing a fun, pleasant, and harassment-free experience for everyone, regardless of gender, gender identity and expression, sexual orientation, disability, physical appearance, body size, race, or religion. In keeping with these aims, we do not tolerate harassment of participants in any form. Participants asked to stop any harassing behavior are expected to comply immediately.\nThis code of conduct applies to all TISL spaces, including mailing lists, chat servers, research meetings, reading groups, and other events, both online and offline. Anyone who violates this code of conduct may be sanctioned or expelled from these spaces at the discretion of the lab director."
  },
  {
    "objectID": "coc.html#no-harassment",
    "href": "coc.html#no-harassment",
    "title": "Lab Code of conduct",
    "section": "No Harassment",
    "text": "No Harassment\nHarassment, including the following activities, will not be tolerated:\n\nOffensive comments related to gender, gender identity and expression, sexual orientation, disability, mental illness, neuro(a)typicality, physical appearance, body size, age, race, or religion.\nUnwelcome comments regarding a person’s lifestyle choices and practices, including those related to food, health, parenting, drugs, and employment.\nDeliberate misgendering or use of ‘dead’ or rejected names.\nGratuitous or off-topic sexual images or behaviour in spaces where they’re not appropriate.\nPhysical contact and simulated physical contact (eg, textual descriptions like “hug” or “backrub“) without consent or after a request to stop.\nThreats of violence.\nIncitement of violence towards any individual, including encouraging a person to commit suicide or to engage in self-harm.\nDeliberate intimidation.\nStalking or following.\nHarassing photography or recording, including logging online activity for harassment purposes.\nSustained disruption of discussion.\nUnwelcome sexual attention.\nPattern of inappropriate social contact, such as requesting/assuming inappropriate levels of intimacy with others. TISL is intended to be a welcoming platform, so while friendships in support of research are encouraged, note that TISL is not a dating platform. Flirting or other unprofessional behavior may make someone uncomfortable. If a member requests another member to stop this type of behavior and it continues, it will be considered harassment.\nContinued one-on-one communication after requests to cease.\nDeliberate “outing” of any aspect of a person’s identity without their consent except as necessary to protect vulnerable people from intentional abuse.\nPublication of non-harassing private communication."
  },
  {
    "objectID": "coc.html#expectation-of-confidentiality",
    "href": "coc.html#expectation-of-confidentiality",
    "title": "Lab Code of conduct",
    "section": "Expectation of Confidentiality",
    "text": "Expectation of Confidentiality\nReasons for confidentiality include:\n\nensuring that ideators and early project contributors receive proper credit for their work before it is published and their names are clearly attached, and\nensuring the validity of double-blind review processes.\n\nExpectations are as follows:\n\n(Confidential channels) When discussing ideas 1:1 or in small group settings, whether via chat, in person, or on a call, do not assume without affirmative consent from someone that you are welcome to share their ideas with others. This means that you should ask if you wish to share, and if they do not agree, or if you have not asked, do not share their ideas or work with anyone.\n(Open channels) In any events advertised to and open to the general public, assume there is no confidentiality. If you are presenting or commenting at such an event: share only ideas, thoughts, and work that you are ok with being copied and modified by others. If you attend such an event and end up finding someone’s ideas or work valuable, feel free to share, but note that the fact that the channel is open does not recuse you from the responsibility to give them credit for their work! Scientific integrity requires that any sharing be prefaced with giving proper credit to the originator of the idea or work, just as with citing published papers."
  },
  {
    "objectID": "coc.html#credit-and-license",
    "href": "coc.html#credit-and-license",
    "title": "Lab Code of conduct",
    "section": "Credit and License",
    "text": "Credit and License\nThis policy is licensed under the Creative Commons Zero license. It is largely adapted from the ML Collective Code of Conduct."
  },
  {
    "objectID": "posts/2019-04-24-fairwashing-in-machine-learning/index.html",
    "href": "posts/2019-04-24-fairwashing-in-machine-learning/index.html",
    "title": "Fairwashing in machine learning",
    "section": "",
    "text": "Machine learning is now used in every aspect of our life, from entertainment to high stakes decision-making processes such as credit scoring, medical diagnosis, or predictive justice. The potential risk of incorrect decisions has raised the public demand for an explanation of the decisions of machine learning models. In addition to this public demand, all across the world, several communities and government initiatives are emerging, asking for more transparency in machine learning models’ decisions, and the development of an ethically-aligned AI. As an example, in Europe, the new General Data Protection Regulation has a provision requiring explanations for the decisions of machine learning models that have a significant impact on individuals (Goodman and Flaxman 2017).\nWe believe that because of this particular combination of regulations and public demand for ethically-aligned development of AI, a dishonest machine learning models’ producers may be tempted to perform fairwashing. We define fairwashing as promoting the false perception that a machine learning model complies with a given ethical requirement while it might not be so. The risk of fairwashing is all the more possible because the right to explanation as defined in current regulations does not give precise directives on what it means to provide a ‘’valid explanation’’ (Wachter, Mittelstadt, and Floridi 2017; Edwards and Veale 2017), leaving a legal loophole that can be exploited by a dishonest model’s producer to cover up a possible misconducts of its black-box model by providing misleading explanations.\nTo demonstrate this risk, we consider two variations of the black-box explanation problem and show that one can forge misleading explanations that comply with a given ethical requirement. In particular, we use fairness as the ethical requirement and demonstrate that given a black-box model that is unfair, one can systematically deduce rule lists with high fidelity to the black-box model while being considerably less unfair at the same time."
  },
  {
    "objectID": "posts/2019-04-24-fairwashing-in-machine-learning/index.html#fairness",
    "href": "posts/2019-04-24-fairwashing-in-machine-learning/index.html#fairness",
    "title": "Fairwashing in machine learning",
    "section": "Fairness ⚖️",
    "text": "Fairness ⚖️\nThere are several definitions for fairness in machine learning (Verma and Rubin 2018). Central to all these definitions is the notion of the sensitive attribute \\(s\\) for which non-discrimination should be established. An example of such attribute can be the gender, ethnicity, or religion. For this work, we use demographic parity (Calders, Kamiran, and Pechenizkiy 2009) as fairness metric. Demographic parity requires the prediction \\(\\hat{y}\\) of the black-box model to be independent of the sensitive attribute \\(s\\), that is, \\(P(\\hat{y}=1 | s= 1) = P(\\hat{y}=1 | s= 0)\\). We therefore define unfairness as \\(\\mathsf{unfairness}=|P(\\hat{y}=1 | s= 1) - P(\\hat{y}=1 | s= 0)|\\)."
  },
  {
    "objectID": "posts/2019-04-24-fairwashing-in-machine-learning/index.html#black-box-explanation",
    "href": "posts/2019-04-24-fairwashing-in-machine-learning/index.html#black-box-explanation",
    "title": "Fairwashing in machine learning",
    "section": "Black-box explanation",
    "text": "Black-box explanation\nBlack-box explanation is the problem of explaining how a machine learning model – whose internal logic is hidden to the auditor and generally complex – produces its outcomes. An explanation can be viewed as an interface between humans and a decision process that is both an accurate proxy of the decision process and understandable by humans (Guidotti et al. 2018). Figure @ref(fig:pipeline) shows an example of a black-box explanation pipeline. First, a black-box model \\(M_b\\) is obtained using a particular learning algorithm (e.g., random forest, SVM, neural network…) on a particular dataset \\(D_{train}\\). Then, the black-box model is used to label another dataset \\(D_{labeled} \\neq D_{train}\\). Finally, an explanation algorithm takes as input \\(D_{labeled}\\) and produces an interpretable model \\(M_i\\). Examples of interfaces accepted in the literature as interpretable models include linear models, decision trees, rule lists, and rule sets. In this work, we use rule lists as explanation models. We consider two variations of the black-box explanation problem, namely model explanation and outcome explanation. The former consists in providing an explanation model that explains all the decisions of the black-box model, while the latter consists of producing an explanation model that explains a particular decision of the black-box model. To assess the performance of an explanation algorithm, we rely on the notion of fidelity (Craven and Shavlik 1996), which is the accuracy of the explanation model \\(M_i\\) relative to the black-box model \\(M_b\\) on some instances \\(X\\). Simply put, \\(\\mathsf{fidelity} = \\frac{1}{|X|} \\sum_{x \\in X} \\mathbb{I}(M_i(x)=M_b(x))\\).\n\n\n\n\n\nBlack-box explanation pipeline"
  },
  {
    "objectID": "posts/2019-04-24-fairwashing-in-machine-learning/index.html#rule-lists",
    "href": "posts/2019-04-24-fairwashing-in-machine-learning/index.html#rule-lists",
    "title": "Fairwashing in machine learning",
    "section": "Rule lists 🗒️",
    "text": "Rule lists 🗒️\nA rule list (Rivest 1987) \\(r= (d_p, \\delta_p, q_0, K)\\) of length \\(K \\geq 0\\) is a \\((K+1)-\\)tuple consisting of \\(K\\) distinct association rules \\(p_k \\to q_k\\), in which \\(p_k \\in d_p\\) is the antecedent of the association rule and \\(q_k \\in \\delta_p\\) its corresponding consequent, followed by a default prediction \\(q_0\\). The rule list below predicts whether a person is likely to make at least 50k per year.\n\n if [capitalGain:&gt;=5119] then [high]\nelse if [maritalStatus:single] then [low]\nelse if [capitalLoss:1882-1978] then [high]\nelse if [workclass:private && education:bachelors] then [high]\nelse if [education:masters_doctorate] then [high]\nelse [low]\n\nTo make a prediction using a rule list, the rules are applied sequentially until one rule applies, in which case the associated outcome is reported. If none of the rules applies, then the default prediction is reported."
  },
  {
    "objectID": "posts/2019-04-24-fairwashing-in-machine-learning/index.html#corels",
    "href": "posts/2019-04-24-fairwashing-in-machine-learning/index.html#corels",
    "title": "Fairwashing in machine learning",
    "section": "CORELS",
    "text": "CORELS\nCORELS (Angelino et al. 2018) is a supervised machine learning algorithm which takes as input a feature vector \\(X\\) and it associated label vector \\(Y\\), all assumed to be binary, and finds the rule list \\(r\\) that minimize the regularized empirical risk: \\(\\mathsf{misc(r, X, Y)} + \\lambda K\\), where \\(\\mathsf{misc(r, X, Y)}\\) is the misclassification error of the rule list and \\(\\lambda \\geq 0\\) is a regularization parameter used to penalize longer rule lists. It represents the search space of the rule lists as a trie and uses an efficient branch-and-bound algorithm to prune it."
  },
  {
    "objectID": "posts/2019-04-24-fairwashing-in-machine-learning/index.html#model-enumeration",
    "href": "posts/2019-04-24-fairwashing-in-machine-learning/index.html#model-enumeration",
    "title": "Fairwashing in machine learning",
    "section": "Model enumeration",
    "text": "Model enumeration\nIn this work, we use a rule list enumeration technique introduced in (Hara and Ishihata 2018). This algorithm takes as input the set \\(T\\) of all binary predictors, and enumerate rule list models in the descending order of their objective function. It maintains a heap \\(\\mathcal{H}\\), whose priority is the objective function value of the rule lists and a list \\(\\mathcal{M}\\) for the enumerated models. First, it starts by computing the optimal rule \\(m=\\mathsf{CORELS}(T) = (d_p, \\delta_p, q_0, K)\\) for \\(T\\) using the CORELS algorithm. Then, it inserts the tuple \\((m, T, \\emptyset)\\) into the heap. Finally, the algorithm repeats the following three steps until the stopping criterion is met: * Extract the tuple \\((m, S, F)\\) with the maximum priority value from \\(\\mathcal{H}\\). * Output \\(m\\) as the \\(i-\\)th model if \\(m \\notin \\mathcal{M}\\). * Branch the search space: compute and insert \\(m' =\\mathsf{CORELS}(S \\setminus \\{t_j\\})\\) into \\(\\mathcal{H}\\) for all \\(t_j \\in \\delta_p\\)."
  },
  {
    "objectID": "posts/2019-04-24-fairwashing-in-machine-learning/index.html#problem-formulation",
    "href": "posts/2019-04-24-fairwashing-in-machine-learning/index.html#problem-formulation",
    "title": "Fairwashing in machine learning",
    "section": "Problem formulation",
    "text": "Problem formulation\nWe define rationalization as the problem of finding an interpretable surrogate model \\(M_i\\) approximating a black-box model \\(M_b\\), such that \\(M_i\\) is fairer than \\(M_b\\). In particular, we distinguish model rationalization and outcome rationalization.\nGiven a black-box model \\(M_b\\), a set of instances \\(X\\) and a sensitive attribute \\(s\\), the model rationalization problem consists in finding an intrepretable global model \\(M_i^g\\) such that \\(\\epsilon(M_i^g, X, s) &gt; \\epsilon(M_b, X, s)\\), for some fairness evaluation metric \\(\\epsilon(\\cdot, \\cdot, \\cdot)\\). The outcome rationalization problem is similar to the model rationalaization problem, but consider only a single data point, and provide an explanation for this particular data point and its neighborhood. More precisely, given a black-box model \\(M_b\\), an instance \\(x\\), a neighborhood \\(\\mathcal{V}(x)\\) of \\(x\\), and a sensitive attribute \\(s\\), the outcome rationalization problem consists in finding an intrepretable local model \\(M_i^l\\) such that \\(\\epsilon(M_i^l, \\mathcal{V}(x), s) &gt; \\epsilon(M_b, \\mathcal{V}(x), s)\\), for some fairness evaluation metric \\(\\epsilon(\\cdot, \\cdot, \\cdot)\\)."
  },
  {
    "objectID": "posts/2019-04-24-fairwashing-in-machine-learning/index.html#laundryml",
    "href": "posts/2019-04-24-fairwashing-in-machine-learning/index.html#laundryml",
    "title": "Fairwashing in machine learning",
    "section": "LaundryML",
    "text": "LaundryML\nWe propose LaundryML, an algorithm to solve the rationalization problem efficiently. First, LaundryML uses a modified version of CORELS in which we define the new objective function as \\(\\mathsf{(1-\\beta) misc(\\cdot)} + \\mathsf{\\beta unfairness(\\cdot)}+ \\lambda K\\), where \\(\\mathsf{misc(\\cdot)}\\) is the misclassification error of the rule list, \\(\\mathsf{unfairness(\\cdot)}\\) returns the unfairness of the rule list, \\(\\lambda \\geq 0\\) is a regularization parameter used to penalize longer rule lists, and \\(\\beta \\geq 0\\) is the weight of the unfairness regularization. Then, it uses the customized CORELS algorithm to enumerated rule list models. Finally, it uses a threshold for the unfairness to select the models that can be used for rationalization.\nTo solve the model rationalization problem for a set \\(X\\) of instances, LaundryML will take as input the tuple \\(T=\\{X, y\\}\\) formed the instances \\(X\\) and the predictions \\(y\\) of the black-box model on \\(X\\). To solve the outcome rationalization for a particular instance \\(x\\), the inputs of LaundryML will be the instances \\(T_x = \\mathsf{neigh(x, T)}\\), some neighborhood searching algorithm \\(\\mathsf{neigh(\\cdot)}\\)."
  },
  {
    "objectID": "posts/2019-04-24-fairwashing-in-machine-learning/index.html#results-for-model-rationalization",
    "href": "posts/2019-04-24-fairwashing-in-machine-learning/index.html#results-for-model-rationalization",
    "title": "Fairwashing in machine learning",
    "section": "Results for model rationalization",
    "text": "Results for model rationalization\nWe perform fairwashaing through model rationalization by explaining the decisions of the black-box model with an interpretable model (here a rule list) that is less unfair than the black-box while having a fidelity greater than \\(90\\%\\). For the model rationalization experiment, we set \\(\\lambda=0.005\\), and \\(\\beta =\\{0, 0.1, 0.2, 0.5, 0.7, 0.9\\}\\), and we enumerate 50 models.\n\n\n\n\n\nCDFs of the unfairness (left) and the fidelity (right) of rationalized explanation models produced by LaundryML on the suing group of Adult Income. Results are for the demographic parity metric and the random forest black-box model. The vertical line on the left figure represents the unfairness of the black-box model. The CDFs on the right figure are the CDFs of the fidelity of explanation models that are fairer than the black box model.\n\n\n\n\nThe results in Figure @ref(fig:global) show that one can obtain fairer explanation models that have good fidelity. In particular, we observe that as \\(\\beta\\) increases, both the unfairness and the fidelity of the enumerated models decrease. Overall, the best model we obtain has a fidelity of \\(0.908\\) and an unfairness of \\(0.058\\). In Figure @ref(fig:audit), we compare the selected rationalization model to the black-box model in term of relative feature dependence ranking. The observations show that the sensitive attribute is ranked \\(2\\)nd (respectively \\(28\\)th) with the black-box model (respectively the rationalization model).\n\n\n\n\n\nRelative feature dependence ranking obtained using FairML to audit models trained on the Adult Income dataset. Green indicates that the feature highly contributes to a high salary rating on Adult. Features that characterize the majority groups are highlighted in yellow. Black-box model (left) vs. LaundryML model (middle) and its description (right)."
  },
  {
    "objectID": "posts/2019-04-24-fairwashing-in-machine-learning/index.html#results-for-outcome-rationalization",
    "href": "posts/2019-04-24-fairwashing-in-machine-learning/index.html#results-for-outcome-rationalization",
    "title": "Fairwashing in machine learning",
    "section": "Results for outcome rationalization",
    "text": "Results for outcome rationalization\nWe perform fairwhasing trough outcome rationalization by explaining each rejected female subject with a rule list whose unfairness (as measured on the neighbourhood of the subject) is lower than that of the black-box model. For the outcome rationalization experiment, we set \\(\\lambda=0.005\\), and \\(\\beta =\\{0.1, 0.3, 0.5, 0.7, 0.9\\}\\), and for each rejected female subjects, we enumerate 50 models to perform outcome rationalization.\n\n\n\n\n\nCDFs of the unfairness of the best model found by LaundryML per user on Adult Income\n\n\n\n\nResults in Figure @ref(fig:local) show that as the fairness regulation parameter \\(\\beta\\) increases, the unfairness of the explanation model decreases. In particular, with \\(\\beta=0.9\\), a completely fair model (unfairness = \\(0.0\\)) was found to explain the outcome for each rejected female subject in Adult Income.\nResults on ProPublica Recidivism dataset, as well as experiments using other black-box models and fairness metrics, are discussed in our paper (Aïvodji et al. 2019)."
  },
  {
    "objectID": "posts/2021-03-29-learning-fair-rulelists-with-faircorels/index.html",
    "href": "posts/2021-03-29-learning-fair-rulelists-with-faircorels/index.html",
    "title": "Learning fair rule lists with FairCORELS",
    "section": "",
    "text": "FairCORELS is an open-source library for learning fair rule lists. In this short tutorial, we will learn how to use it to train rule lists under group fairness constraints."
  },
  {
    "objectID": "posts/2021-03-29-learning-fair-rulelists-with-faircorels/index.html#group-fairness",
    "href": "posts/2021-03-29-learning-fair-rulelists-with-faircorels/index.html#group-fairness",
    "title": "Learning fair rule lists with FairCORELS",
    "section": "Group fairness",
    "text": "Group fairness\nSeveral notions of fairness have been proposed in the machine learning literature (Narayanan 2018; Chouldechova and Roth 2018). This tutorial will focus on statistical notions of fairness (Verma and Rubin 2018), also known as group fairness definitions. Simply put, statistical notions of fairness aim at studying how the performances of a machine learning model, as evaluated by a statistical measure \\(m\\) (e.g., false positive or false negative rates), differ depending on the group membership of individuals within a dataset. Group memberships are often determined by the values of particular attributes (e.g., gender, age, ethnicity) hereafter referred to as sensitive attributes. In this context, the word unfairness is usually used to characterize the gap between the value of \\(m\\) for a majority group (e.g., historically advantaged individuals) and its value for a minority group (e.g., historically disadvantaged individuals). Table 1 summarizes some of the most frequently used statistical notions of fairness.\n\n\n\n\n\n\n\nname\nmeasure\nidentifier\nmethod\n\n\n\n\nstatistical parity (SP)\nprobability of receiving a positive outcome\n1\nstatistical_parity()\n\n\npredictive parity (PP)\npositive predictive value\n2\npredictive_parity()\n\n\npredictive equality (PE)\nfalse positive rate\n3\npredictive_equality()\n\n\nequal opportunity (EOpp)\nfalse negative rate\n4\npredictive_equality()\n\n\nequalized odds (EOdds)\nfalse negative rate and false positive rate\n5\nequalized_odds()\n\n\nconditional use accuracy equality (CUAE)\npositive predictive value and negative predictive value\n6\nconditional_use_accuracy_equality()\n\n\n\nTable 1: Summary of statistical notions of fairness with the related statistical measure involved. The columns identifier and method represent the id and the method used for the fairness metric within the FairCORELS library."
  },
  {
    "objectID": "posts/2021-03-29-learning-fair-rulelists-with-faircorels/index.html#rule-lists",
    "href": "posts/2021-03-29-learning-fair-rulelists-with-faircorels/index.html#rule-lists",
    "title": "Learning fair rule lists with FairCORELS",
    "section": "Rule lists",
    "text": "Rule lists\nA rule list (Rivest 1987; Angelino et al. 2018) \\(r= (d_p, \\delta_p, q_0, K)\\) of length \\(K \\geq 0\\) is a \\((K+1)-\\)tuple consisting of \\(K\\) distinct association rules \\(p_k \\to q_k\\), in which \\(p_k \\in d_p\\) is the antecedent of the association rule and \\(q_k \\in \\delta_p\\) its corresponding consequent, followed by a default prediction \\(q_0\\). The rule list below predicts whether a person is likely to make at least 50k per year.\n\n if [capitalGain:&gt;=5119] then [high]\nelse if [maritalStatus:single] then [low]\nelse if [capitalLoss:1882-1978] then [high]\nelse if [workclass:private && education:bachelors] then [high]\nelse if [education:masters_doctorate] then [high]\nelse [low]\n\nTo make a prediction using a rule list, the rules are applied sequentially until one rule applies, in which case the associated outcome is reported. If none of the rules applies, then the default prediction is reported."
  },
  {
    "objectID": "posts/2021-03-29-learning-fair-rulelists-with-faircorels/index.html#corels",
    "href": "posts/2021-03-29-learning-fair-rulelists-with-faircorels/index.html#corels",
    "title": "Learning fair rule lists with FairCORELS",
    "section": "CORELS",
    "text": "CORELS\nCORELS (Angelino et al. 2018) is a supervised machine learning algorithm which takes as input a feature vector \\(X\\) and its associated label vector \\(Y\\), all assumed to be binary, and finds the rule list \\(r\\) that minimize the regularized empirical risk: \\(\\mathsf{misc(r, X, y)} + \\lambda K\\), where \\(\\mathsf{misc(r, X, y)}\\) is the misclassification error of the rule list and \\(\\lambda \\geq 0\\) is a regularization parameter used to penalize longer rule lists. It represents the search space of the rule lists as a trie and uses an efficient branch-and-bound algorithm to prune it."
  },
  {
    "objectID": "posts/2021-03-29-learning-fair-rulelists-with-faircorels/index.html#faircorels",
    "href": "posts/2021-03-29-learning-fair-rulelists-with-faircorels/index.html#faircorels",
    "title": "Learning fair rule lists with FairCORELS",
    "section": "FairCORELS",
    "text": "FairCORELS\nFairCORELS (Aïvodji et al. 2019) is a multi-objective formulation of CORELS that aims to learn fair rule lists. More precisely, FairCORELS solves the following optimization problem:\n\\[\\begin{eqnarray}\n\\label{eq:faircorels}\n     \n     \\underset{r \\in \\mathcal{R}}{\\operatorname{argmin}}   & \\mathsf{misc(r, X, y)} + \\lambda K  \\\\\n    \\text{s.t. }   & \\texttt{unf}(r,X,y, A) \\leq \\epsilon, \\\\\n\\end{eqnarray}\\]\nwhere the objective function is the regularized empirical risk of CORELS and the constraint requires that the unfairness \\(unf(r,X,y, A)\\) of the solution, for a particular sensitive attribute \\(A\\), to be less than a value \\(\\epsilon\\)."
  },
  {
    "objectID": "posts/2021-03-29-learning-fair-rulelists-with-faircorels/index.html#installation",
    "href": "posts/2021-03-29-learning-fair-rulelists-with-faircorels/index.html#installation",
    "title": "Learning fair rule lists with FairCORELS",
    "section": "Installation",
    "text": "Installation\nFairCORELS can be installed easily using pip install faircorels. Read the README for more details on the installation process."
  },
  {
    "objectID": "posts/2021-03-29-learning-fair-rulelists-with-faircorels/index.html#dataset-and-preprocessing",
    "href": "posts/2021-03-29-learning-fair-rulelists-with-faircorels/index.html#dataset-and-preprocessing",
    "title": "Learning fair rule lists with FairCORELS",
    "section": "Dataset and Preprocessing",
    "text": "Dataset and Preprocessing\nFor this tutorial, we will use the Adult Income dataset. It contains demographic information of about \\(48,842\\) individuals from the \\(1994\\) U.S. census. The associated classification task consists of predicting whether a particular individual earns more than \\(50,000\\$\\) per year. Table 2 shows the first six records of the dataset.\n\n\n\n\nTable 2: First six rows of the Adult Income dataset.\n\n\nage\nworkclass\nfnlwgt\neducation\neducational_num\nmarital_status\noccupation\nrelationship\nrace\ngender\ncapital_gain\ncapital_loss\nhours_per_week\nnative_country\nincome\n\n\n\n\n25\nPrivate\n226802\n11th\n7\nNever-married\nMachine-op-inspct\nOwn-child\nBlack\nMale\n0\n0\n40\nUnited-States\n&lt;=50K\n\n\n38\nPrivate\n89814\nHS-grad\n9\nMarried-civ-spouse\nFarming-fishing\nHusband\nWhite\nMale\n0\n0\n50\nUnited-States\n&lt;=50K\n\n\n28\nLocal-gov\n336951\nAssoc-acdm\n12\nMarried-civ-spouse\nProtective-serv\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n&gt;50K\n\n\n44\nPrivate\n160323\nSome-college\n10\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nBlack\nMale\n7688\n0\n40\nUnited-States\n&gt;50K\n\n\n34\nPrivate\n198693\n10th\n6\nNever-married\nOther-service\nNot-in-family\nWhite\nMale\n0\n0\n30\nUnited-States\n&lt;=50K\n\n\n63\nSelf-emp-not-inc\n104626\nProf-school\n15\nMarried-civ-spouse\nProf-specialty\nHusband\nWhite\nMale\n3103\n0\n32\nUnited-States\n&gt;50K\n\n\n\n\n\n\n\n\nSimilar to CORELS, FairCORELS also need input data to be categorical. The dataset will be transformed as follows:\n\nFirst, all the numerical attributes are converted into categorical ones by using the Minimum Description Length Principle (Fayyad and Irani 1993).\nAfterwards, one-hot encoding is applied to the resulting dataset.\nFinally, the set of rules (composed of single-clause, negative single-clause, and two-clauses rules) is constructed by applying FPGrowth (Han, Pei, and Yin 2000) to the one-hot encoded dataset.\n\nTable 3 gives an overview of the preprocessed dataset.\n\n\n\n\nTable 3: First six rows of the binarized dataset.\n\n\nworkclass.fedGov\nworkclass.otherGov\nworkclass.private\nworkclass.selfEmployed\nworkclass.unEmployed\neducation.associates\neducation.bachelors\neducation.dropout\neducation.hs_grad\neducation.masters_doctorate\neducation.prof_school\nmarital_status.married\nmarital_status.single\noccupation.blueCollar\noccupation.other\noccupation.professional\noccupation.sales\noccupation.whiteCollar\ngender.Female\ngender.Male\nage..20.5\nage.20.5.23.5\nage.23.5.24.5\nage.24.5.27.5\nage.27.5.29.5\nage.29.5.35.5\nage.35.5.41.5\nage.41.5.54.5\nage.54.5.61.5\nage..61.5\ncapital_gain..57.0\ncapital_gain.57.0.3048.0\ncapital_gain.3048.0.3120.0\ncapital_gain.3120.0.4243.5\ncapital_gain.4243.5.4401.0\ncapital_gain.4401.0.4668.5\ncapital_gain.4668.5.4826.0\ncapital_gain.4826.0.4932.5\ncapital_gain.4932.5.4973.5\ncapital_gain.4973.5.5119.0\ncapital_gain.5119.0.5316.5\ncapital_gain.5316.5.5505.5\ncapital_gain.5505.5.6618.5\ncapital_gain.6618.5.7055.5\ncapital_gain..7055.5\ncapital_loss..1551.5\ncapital_loss.1551.5.1568.5\ncapital_loss.1568.5.1820.5\ncapital_loss.1820.5.1859.0\ncapital_loss.1859.0.1881.5\ncapital_loss.1881.5.1894.5\ncapital_loss.1894.5.1927.5\ncapital_loss.1927.5.1975.5\ncapital_loss.1975.5.1978.5\ncapital_loss.1978.5.2168.5\ncapital_loss.2168.5.2176.5\ncapital_loss.2176.5.2218.5\ncapital_loss.2218.5.2252.0\ncapital_loss.2252.0.2384.5\ncapital_loss.2384.5.2450.5\ncapital_loss.2450.5.2469.5\ncapital_loss.2469.5.3089.5\ncapital_loss..3089.5\nhours_per_week..34.5\nhours_per_week.34.5.39.5\nhours_per_week.39.5.41.5\nhours_per_week.41.5.49.5\nhours_per_week.49.5.61.5\nhours_per_week..61.5\nincome\n\n\n\n\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n\n\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n\n\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0"
  },
  {
    "objectID": "posts/2021-03-29-learning-fair-rulelists-with-faircorels/index.html#training",
    "href": "posts/2021-03-29-learning-fair-rulelists-with-faircorels/index.html#training",
    "title": "Learning fair rule lists with FairCORELS",
    "section": "Training",
    "text": "Training\nTo train the constrained rule list, we will need the following libraries:\n\npandas: to load the dataset.\nsklearn: to split the dataset into train and test sets.\nfaircorels: to train the rule list with fairness constraint and to evaluate the unfairness and the accuracy\n\nFirst, let’s load the libraries!\n\nimport pandas as pd\nfrom faircorels import FairCorelsClassifier, ConfusionMatrix, Metric\nfrom sklearn.model_selection import train_test_split\n\nNow, we will perform the following tasks:\n\nLoad the preprocessed dataset.\nSeparate the feature vector from the label vector.\nRemove the feature vector that characterizes the majority and the minority groups. In fact, the sensitive attribute is only used to enforce the fairness contraint at training time. The sensitive attribute is not used at inference time to avoid disparate treatment.\nSplit the dataset into train and test sets.\n\n\n# loading the preprocessed dataset\ndf = pd.read_csv(\"../../../datasets/adult_income/binary.csv\")\n\n# getting the label vector\ny = df[\"income\"]\ny = y.to_numpy()\n\n\n# getting the majority and minority groups\nmaj_group = df[\"gender:Male\"]\nmin_group = df[\"gender:Female\"]\n\n#getting the rest ot the features \nX = df.drop([\"income\", \"gender:Male\", \"gender:Female\"], axis=1)\nfeatures = X.columns.tolist()\nX = X.to_numpy()\n\n# split the dataset into train/test sets\nX_train, X_test, y_train, y_test, \\\nmaj_group_train, maj_group_test, \\\nmin_group_train, min_group_test =  train_test_split(X, y, \n                                                    maj_group, \n                                                    min_group,\n                                                    test_size=0.33, \n                                                    random_state=42)\n\nFinally, we can train the rule list.\n\n# create the model\nclf = FairCorelsClassifier( \n                        c = 1e-3,\n                        mode = 3,\n                        fairness = 4,\n                        epsilon = 0.95,\n                        max_card = 1,\n                        maj_vect = maj_group_train,\n                        min_vect = min_group_train,\n                        n_iter = 300000, \n                        policy = \"bfs\",\n                        bfs_mode = 2,\n                        verbosity = [])\n# train the model\nclf.fit(X_train, y_train, features=features, prediction_name=\"[income:&gt;50K]\")\n\nThe rule list model is instantiated using the FairCorelsClassifier class with the following parameters:\n\nc: is the weight of the rule list’s length regularizer.\nmode: determines the optimization methods. A value of 3 corresponds to the epsilon constraint method. That is, it will fix a value for the unfairness and then maximize the accuracy.\nfairness: identifier of the fairness metric used. See Table @ref(tab:groupfairness) to get the identifier of each fairness metric and their corresponding methods. For this example, we used the equal opportunity metric.\nepsilon: is the value of the fairness constraint. So, a value of 0.95 means that we want an unfairness gap of 0.05.\nmax_card: maximum cardinality allowed when mining rules. In this example, we use max_card = 1 since we have already performed the mining with FPGrowth.\nmaj_vect and min_vect: are used to specify the majority and minority groups\nn_iter: is used to specify the maximum number of nodes (rule lists) to search before exiting. Even if branch-and-bound is an exact method, we will need to stop the search algorithm after a certain number of iterations to avoid high computational overhead. However, whenever the algorithm stops, we have the guarantee that the best rule list given the computational constraint is returned.\npolicy and bfs_mode: are used to specify, the exploration strategy. That is, how nodes are ordered within the priority queue of the branch-and-bound framework. Here, we use a breadth-first search and nodes are selected according to the value of the objective function of the rule lists that they represent.\n\nThe rule list is trained with the fit method."
  },
  {
    "objectID": "posts/2021-03-29-learning-fair-rulelists-with-faircorels/index.html#evaluation-of-the-rule-list",
    "href": "posts/2021-03-29-learning-fair-rulelists-with-faircorels/index.html#evaluation-of-the-rule-list",
    "title": "Learning fair rule lists with FairCORELS",
    "section": "Evaluation of the rule list",
    "text": "Evaluation of the rule list\nTo obtain the trained model’s description, we can use the rl_ attribute of the rule list object.\n\nprint(clf.rl_)\n\nWe can compute the accuracy of the model by using the score method.\n\naccuracy_train = clf.score(X_train, y_train)\naccuracy_test = clf.score(X_test, y_test)\n\n\nprint(\"Accuracy train: {}\".format(accuracy_train))\n\n\nprint(\"Accuracy test: {}\".format(accuracy_test))\n\nTo compute the unfairness, we will use the ConfusionMatrix and Metric classes.\n\ndef compute_unfairness(min_group, maj_group, X, y):\n  cm = ConfusionMatrix(min_group, maj_group, clf.predict(X), y)\n  cm_minority, cm_majority = cm.get_matrix()\n  fairness_metric = Metric(cm_minority, cm_majority)\n  return fairness_metric\n\nfairness_metric_train = compute_unfairness(min_group_train, maj_group_train, \n                                            X_train, y_train)\nfairness_metric_test  = compute_unfairness(min_group_test, maj_group_test, \n                                            X_test, y_test)\n\n\nprint(\"Unfairness train: {}\".format(fairness_metric_train.equal_opportunity()))\n\n\nprint(\"Unfairness test: {}\".format(fairness_metric_test.equal_opportunity()))                                                         \n\nTo evaluate the unfairness, we first create a confusion matrix for both the minority and majority groups using the ConfusionMatrix class. Then, we create a metric object using Metric. Finally, we get the value of the unfairness using the corresponding method equal_opportunity().\nIn this particular example, we can see that the learned rule list effectively has an unfairness that is lower than 0.05 and an accuracy of 0.819 on the training set. It also generalizes well: it has an unfairness close to 0.00 and an accuracy of 0.816 on the test set."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TISL",
    "section": "",
    "text": "The Trustworthy Information Systems Lab (TISL) tackles questions related to the design and deployment of trustworthy information and communication systems. Our research areas of interest are computer security, data privacy, optimization, and machine learning. Current research investigations of the group include data privacy and several aspects of trustworthy machine learning, such as fairness, privacy-preserving machine learning, and explainability."
  },
  {
    "objectID": "index.html#members",
    "href": "index.html#members",
    "title": "TISL",
    "section": "🧸 Members",
    "text": "🧸 Members\n\n  Dave-Harold Mbiazi-Njanda, MSc Student\n    ☘️ co-supervision with \n  Foutse Khomh\n  \n  Eliott Baltz, MSc Student\n  \n  Maryam Babaie, PhD Student\n    ☘️ co-supervision with \n  Sébastien Gambs\n  \n  Meghana Bhange, MSc Student\n    ☘️ co-supervision with \n  Jean-Marc Robert\n  \n  Patrik Kenfack, PhD Student\n    ☘️ co-supervision with \n  Samira Ebrahimi Kahou\n  \n  Ulrich Aïvodji, Faculty\n  \n\nNo matching items"
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "TISL",
    "section": "📜 Code of Conduct",
    "text": "📜 Code of Conduct\nTISL is committed to ensuring an enjoyable, positive, and respectful experience for all. We anticipate the collaboration of every member to maintain a secure environment for everyone. For a more detailed version, please refer to the Code of Conduct page."
  },
  {
    "objectID": "index.html#funding",
    "href": "index.html#funding",
    "title": "TISL",
    "section": "💰 Funding",
    "text": "💰 Funding\nTISL is supported by the following organizations:\n\nÉcole de technologie supérieure\nCompute Canada\nNSERC\nCIFAR"
  },
  {
    "objectID": "index.html#alumni",
    "href": "index.html#alumni",
    "title": "TISL",
    "section": "🚀 Alumni",
    "text": "🚀 Alumni\n\n  Darius Harold Touko Tchakoua, MSc Student, Non-thesis\n  \n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          year (Low to High)\n        \n         \n          year (High to Low)\n        \n         \n          venue\n        \n     \n  \n    \n      \n      \n    \n\n\n      \n        Exploiting Fairness to Enhance Sensitive Attributes Reconstruction. \n        Julien Ferry, Ulrich Aïvodji, Sébastien Gambs, Marie-José Huguet, Mohamed Siala. IEEE Conference on Secure and Trustworthy Machine Learning - SaTML, 2023.\n      \n      \n        Fooling SHAP with Stealthily Biased Sampling. \n        Gabriel Laberge, Ulrich Aïvodji, Satoshi Hara, Mario Marchand, Foutse Khomh. International Conference on Learning Representations - ICLR, 2023.\n      \n      \n        Leveraging Integer Linear Programming to Learn Optimal Fair Rule Lists. \n        Julien Ferry, Ulrich Aïvodji, Sébastien Gambs, Marie-José Huguet, Mohamed Siala. International Conference on the Integration of Constraint Programming, Artificial Intelligence, and Operations Research - CPAIOR, 2022.\n      \n      \n        Fairness via Explanation Quality: Evaluating Disparities in the Quality of Post hoc Explanations. \n        Jessica Dai, Sohini Upadhyay, Ulrich Aïvodji, Stephen H. Bach, Himabindu Lakkaraju. AAAI/ACM Conference on AI, Ethics, and Society - AIES, 2022.\n      \n      \n        Washing The Unwashable: On The (Im) possibility of Fairwashing Detection. \n        Ali Shahin Shamsabadi, Mohammad Yaghini, Natalie Dullerud, Sierra Wyllie, Ulrich Aïvodji, Aisha Alaagib Alryeh Mkean, Sébastien Gambs, Nicolas Papernot. Conference on Neural Information Processing Systems - NeurIPS, 2022.\n      \n      \n        Learning-based Incast Performance Inference in Software-Defined Data Centers. \n        Benoit Nougnanke, Yann Labit, Marc Bruyere, Simone Ferlin, Ulrich Aïvodji. Conference on Innovation in Clouds, Internet and Networks - ICIN, 2021.\n      \n      \n        FairCORELS, an Open-Source Library for Learning Fair Rule Lists. \n        Ulrich Aïvodji, Julien Ferry, Sébastien Gambs, Marie-José Huguet, Mohamed Siala. ACM International Conference on Information and Knowledge Management - CIKM, Demo Track, 2021.\n      \n      \n        Characterizing the risk of fairwashing. \n        Ulrich Aïvodji, Hiromi Arai, Sébastien Gambs, Satoshi Hara. Conference on Neural Information Processing Systems - NeurIPS, 2021.\n      \n      \n        Fairwashing: the risk of rationalization. \n        Ulrich Aïvodji, Hiromi Arai, Olivier Fortineau, Sébastien Gambs, Satoshi Hara, Alain Tapp. International Conference on Machine Learning - ICML, 2019.\n      \n      \n        Sride: A privacy-preserving ridesharing system. \n        Ulrich Aïvodji, Kévin Huguenin, Marie-José Huguet, Marc-Olivier Killijian. ACM Conference on Security & Privacy in Wireless and Mobile Networks - WiSec, 2018.\n      \n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          year (Low to High)\n        \n         \n          year (High to Low)\n        \n         \n          venue\n        \n     \n  \n    \n      \n      \n    \n\n\n      \n        Meeting points in ridesharing: A privacy-preserving approach. \n        Ulrich Aïvodji, Sébastien Gambs, Marie-José Huguet, Marc-Olivier Killijian. Transportation Research Part C: Emerging Technologies, 2016.\n      \n      \n        Improving Fairness Generalization Through a Sample-Robust Optimization Method. \n        Julien Ferry, Ulrich Aïvodji, Sébastien Gambs, Marie-José Huguet, Mohamed Siala. Machine learning, 2022.\n      \n      \n        ML-based Performance Modeling in SDN-enabled Data Center Networks. \n        Benoit Nougnanke, Yann Labit, Marc Bruyere, Ulrich Aïvodji, Simone Ferlin. IEEE Transactions on Network and Service Management, 2022.\n      \n      \n        Local Data Debiasing for Fairness Based on Generative Adversarial Training. \n        Ulrich Aïvodji, François Bidet, Sébastien Gambs, Rosin C. Ngueveu, Alain Tapp. Algorithms, 2021.\n      \n      \n        Privacy in trajectory micro-data publishing: a survey. \n        Marco Fiore, Panagiota Katsikouli, Elli Zavou, Mathieu Cunche, Françoise Fessant, Dominique Le Hello, Ulrich Aïvodji, Baptiste Olivier, Tony Quertier, Razvan Stanica. Transactions on Data Privacy, 2020.\n      \n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          year (Low to High)\n        \n         \n          year (High to Low)\n        \n         \n          venue\n        \n     \n  \n    \n      \n      \n    \n\n\n\n      \n        Privacy-Preserving route synchronization for ridesharing. \n        Ulrich Aïvodji, Sébastien Gambs, Marie-José Huguet, Marc-Olivier Killijian. Atelier sur la Protection de la Vie Privée, 2015.\n      \n      \n        Privacy-preserving carpooling. \n        Ulrich Aïvodji, Sébastien Gambs, Marie-José Huguet, Marc-Olivier Killijian. International Workshop on Freight Transportation and Logistics, 2015.\n      \n      \n        Covoiturage respectueux de la vie privée. \n        Ulrich Aïvodji, Sébastien Gambs, Marie-José Huguet, Marc-Olivier Killijian. Congrès annuel de la Société Française de Recherche Opérationnelle et d'Aide à la Décision, 2016.\n      \n      \n        Privacy-Preserving matching for ridesharing. \n        Ulrich Aïvodji, Kévin Huguenin, Marie-José Huguet, Marc-Olivier Killijian. Atelier sur la Protection de la Vie Privée, 2017.\n      \n      \n        SRide: A Privacy-Preserving Ridesharing Method. \n        Ulrich Aïvodji, Marie-José Huguet, Marc-Olivier Killijian. Congrès annuel de la Société Française de Recherche Opérationnelle et d'Aide à la Décision, 2018.\n      \n      \n        IOTFLA : A secured and privacy-preserving smart home architecture implementing federated learning. \n        Ulrich Aïvodji, Sébastien Gambs, Alexandre Martin. IEEE Symposium on Security and Privacy Workshops - SPW, 2019.\n      \n      \n        GAMIN: An Adversarial Approach to Black-Box Model Inversion. \n        Ulrich Aïvodji, Sébastien Gambs, Timon Ther.. AAAI Workshop on Privacy-Preserving Artificial Intelligence, 2020.\n      \n      \n        Optimisation Distributionnellement Robuste pour améliorer la généralisation de l'équité en apprentissage. \n        Julien Ferry, Ulrich Aïvodji, Sébastien Gambs, Marie-José Huguet, Mohamed Siala. Congrès annuel de la Société Française de Recherche Opérationnelle et d'Aide à la Décision, 2021.\n      \n      \n        Concilier l'équité statistique et la précision en apprentissage machine interprétable grâce à la PLNE. \n        Julien Ferry, Ulrich Aïvodji, Sébastien Gambs, Marie-José Huguet, Mohamed Siala. Congrès annuel de la Société Française de Recherche Opérationnelle et d'Aide à la Décision, 2022.\n      \n      \n        Améliorer la généralisation de l'équité en apprentissage grâce à l'Optimisation Distributionnellement Robuste.. \n        Julien Ferry, Ulrich Aïvodji, Sébastien Gambs, Marie-José Huguet, Mohamed Siala. Rencontres des Jeunes Chercheurs en Intelligence Artificielle, 2021.\n      \n      \n        Exploiter l'équité d'un modèle d'apprentissage pour reconstruire les attributs sensibles de son ensemble d'entraînement. \n        Julien Ferry, Ulrich Aïvodji, Sébastien Gambs, Marie-José Huguet, Mohamed Siala. Rencontres des Jeunes Chercheurs en Intelligence Artificielle, 2023.\n      \n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          year (Low to High)\n        \n         \n          year (High to Low)\n        \n     \n  \n    \n      \n      \n    \n\n\n\n      \n        Probabilistic Dataset Reconstruction from Interpretable Models. \n        Julien Ferry, Ulrich Aïvodji, Sébastien Gambs, Marie-José Huguet, Mohamed Siala. , 2023.\n      \n      \n        Fairness Under Demographic Scarce Regime. \n        Patrik Joslin Kenfack, Samira Ebrahimi Kahou, Ulrich Aïvodji. , 2023.\n      \n      \n        Learning Hybrid Interpretable Models: Theory, Taxonomy, and Methods. \n        Julien Ferry, Gabriel Laberge, Ulrich Aïvodji. , 2023.\n      \n      \n        Model extraction from counterfactual explanations. \n        Ulrich Aïvodji, Alexandre Bolot, Sébastien Gambs. , 2020.\n      \n      \n        Learning fair rule lists. \n        Julien Ferry, Ulrich Aïvodji, Sébastien Gambs, Marie-José Huguet, Mohamed Siala. , 2019.\n      \n\n\nNo matching items"
  },
  {
    "objectID": "publications.html#publications",
    "href": "publications.html#publications",
    "title": "Publications",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          year (Low to High)\n        \n         \n          year (High to Low)\n        \n         \n          venue\n        \n     \n  \n    \n      \n      \n    \n\n\n      \n        Exploiting Fairness to Enhance Sensitive Attributes Reconstruction. \n        Julien Ferry, Ulrich Aïvodji, Sébastien Gambs, Marie-José Huguet, Mohamed Siala. IEEE Conference on Secure and Trustworthy Machine Learning - SaTML, 2023.\n      \n      \n        Fooling SHAP with Stealthily Biased Sampling. \n        Gabriel Laberge, Ulrich Aïvodji, Satoshi Hara, Mario Marchand, Foutse Khomh. International Conference on Learning Representations - ICLR, 2023.\n      \n      \n        Leveraging Integer Linear Programming to Learn Optimal Fair Rule Lists. \n        Julien Ferry, Ulrich Aïvodji, Sébastien Gambs, Marie-José Huguet, Mohamed Siala. International Conference on the Integration of Constraint Programming, Artificial Intelligence, and Operations Research - CPAIOR, 2022.\n      \n      \n        Fairness via Explanation Quality: Evaluating Disparities in the Quality of Post hoc Explanations. \n        Jessica Dai, Sohini Upadhyay, Ulrich Aïvodji, Stephen H. Bach, Himabindu Lakkaraju. AAAI/ACM Conference on AI, Ethics, and Society - AIES, 2022.\n      \n      \n        Washing The Unwashable: On The (Im) possibility of Fairwashing Detection. \n        Ali Shahin Shamsabadi, Mohammad Yaghini, Natalie Dullerud, Sierra Wyllie, Ulrich Aïvodji, Aisha Alaagib Alryeh Mkean, Sébastien Gambs, Nicolas Papernot. Conference on Neural Information Processing Systems - NeurIPS, 2022.\n      \n      \n        Learning-based Incast Performance Inference in Software-Defined Data Centers. \n        Benoit Nougnanke, Yann Labit, Marc Bruyere, Simone Ferlin, Ulrich Aïvodji. Conference on Innovation in Clouds, Internet and Networks - ICIN, 2021.\n      \n      \n        FairCORELS, an Open-Source Library for Learning Fair Rule Lists. \n        Ulrich Aïvodji, Julien Ferry, Sébastien Gambs, Marie-José Huguet, Mohamed Siala. ACM International Conference on Information and Knowledge Management - CIKM, Demo Track, 2021.\n      \n      \n        Characterizing the risk of fairwashing. \n        Ulrich Aïvodji, Hiromi Arai, Sébastien Gambs, Satoshi Hara. Conference on Neural Information Processing Systems - NeurIPS, 2021.\n      \n      \n        Fairwashing: the risk of rationalization. \n        Ulrich Aïvodji, Hiromi Arai, Olivier Fortineau, Sébastien Gambs, Satoshi Hara, Alain Tapp. International Conference on Machine Learning - ICML, 2019.\n      \n      \n        Sride: A privacy-preserving ridesharing system. \n        Ulrich Aïvodji, Kévin Huguenin, Marie-José Huguet, Marc-Olivier Killijian. ACM Conference on Security & Privacy in Wireless and Mobile Networks - WiSec, 2018.\n      \n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          year (Low to High)\n        \n         \n          year (High to Low)\n        \n         \n          venue\n        \n     \n  \n    \n      \n      \n    \n\n\n      \n        Meeting points in ridesharing: A privacy-preserving approach. \n        Ulrich Aïvodji, Sébastien Gambs, Marie-José Huguet, Marc-Olivier Killijian. Transportation Research Part C: Emerging Technologies, 2016.\n      \n      \n        Improving Fairness Generalization Through a Sample-Robust Optimization Method. \n        Julien Ferry, Ulrich Aïvodji, Sébastien Gambs, Marie-José Huguet, Mohamed Siala. Machine learning, 2022.\n      \n      \n        ML-based Performance Modeling in SDN-enabled Data Center Networks. \n        Benoit Nougnanke, Yann Labit, Marc Bruyere, Ulrich Aïvodji, Simone Ferlin. IEEE Transactions on Network and Service Management, 2022.\n      \n      \n        Local Data Debiasing for Fairness Based on Generative Adversarial Training. \n        Ulrich Aïvodji, François Bidet, Sébastien Gambs, Rosin C. Ngueveu, Alain Tapp. Algorithms, 2021.\n      \n      \n        Privacy in trajectory micro-data publishing: a survey. \n        Marco Fiore, Panagiota Katsikouli, Elli Zavou, Mathieu Cunche, Françoise Fessant, Dominique Le Hello, Ulrich Aïvodji, Baptiste Olivier, Tony Quertier, Razvan Stanica. Transactions on Data Privacy, 2020.\n      \n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          year (Low to High)\n        \n         \n          year (High to Low)\n        \n         \n          venue\n        \n     \n  \n    \n      \n      \n    \n\n\n\n      \n        Privacy-Preserving route synchronization for ridesharing. \n        Ulrich Aïvodji, Sébastien Gambs, Marie-José Huguet, Marc-Olivier Killijian. Atelier sur la Protection de la Vie Privée, 2015.\n      \n      \n        Privacy-preserving carpooling. \n        Ulrich Aïvodji, Sébastien Gambs, Marie-José Huguet, Marc-Olivier Killijian. International Workshop on Freight Transportation and Logistics, 2015.\n      \n      \n        Covoiturage respectueux de la vie privée. \n        Ulrich Aïvodji, Sébastien Gambs, Marie-José Huguet, Marc-Olivier Killijian. Congrès annuel de la Société Française de Recherche Opérationnelle et d'Aide à la Décision, 2016.\n      \n      \n        Privacy-Preserving matching for ridesharing. \n        Ulrich Aïvodji, Kévin Huguenin, Marie-José Huguet, Marc-Olivier Killijian. Atelier sur la Protection de la Vie Privée, 2017.\n      \n      \n        SRide: A Privacy-Preserving Ridesharing Method. \n        Ulrich Aïvodji, Marie-José Huguet, Marc-Olivier Killijian. Congrès annuel de la Société Française de Recherche Opérationnelle et d'Aide à la Décision, 2018.\n      \n      \n        IOTFLA : A secured and privacy-preserving smart home architecture implementing federated learning. \n        Ulrich Aïvodji, Sébastien Gambs, Alexandre Martin. IEEE Symposium on Security and Privacy Workshops - SPW, 2019.\n      \n      \n        GAMIN: An Adversarial Approach to Black-Box Model Inversion. \n        Ulrich Aïvodji, Sébastien Gambs, Timon Ther.. AAAI Workshop on Privacy-Preserving Artificial Intelligence, 2020.\n      \n      \n        Optimisation Distributionnellement Robuste pour améliorer la généralisation de l'équité en apprentissage. \n        Julien Ferry, Ulrich Aïvodji, Sébastien Gambs, Marie-José Huguet, Mohamed Siala. Congrès annuel de la Société Française de Recherche Opérationnelle et d'Aide à la Décision, 2021.\n      \n      \n        Concilier l'équité statistique et la précision en apprentissage machine interprétable grâce à la PLNE. \n        Julien Ferry, Ulrich Aïvodji, Sébastien Gambs, Marie-José Huguet, Mohamed Siala. Congrès annuel de la Société Française de Recherche Opérationnelle et d'Aide à la Décision, 2022.\n      \n      \n        Améliorer la généralisation de l'équité en apprentissage grâce à l'Optimisation Distributionnellement Robuste.. \n        Julien Ferry, Ulrich Aïvodji, Sébastien Gambs, Marie-José Huguet, Mohamed Siala. Rencontres des Jeunes Chercheurs en Intelligence Artificielle, 2021.\n      \n      \n        Exploiter l'équité d'un modèle d'apprentissage pour reconstruire les attributs sensibles de son ensemble d'entraînement. \n        Julien Ferry, Ulrich Aïvodji, Sébastien Gambs, Marie-José Huguet, Mohamed Siala. Rencontres des Jeunes Chercheurs en Intelligence Artificielle, 2023.\n      \n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          year (Low to High)\n        \n         \n          year (High to Low)\n        \n     \n  \n    \n      \n      \n    \n\n\n\n      \n        Probabilistic Dataset Reconstruction from Interpretable Models. \n        Julien Ferry, Ulrich Aïvodji, Sébastien Gambs, Marie-José Huguet, Mohamed Siala. , 2023.\n      \n      \n        Fairness Under Demographic Scarce Regime. \n        Patrik Joslin Kenfack, Samira Ebrahimi Kahou, Ulrich Aïvodji. , 2023.\n      \n      \n        Learning Hybrid Interpretable Models: Theory, Taxonomy, and Methods. \n        Julien Ferry, Gabriel Laberge, Ulrich Aïvodji. , 2023.\n      \n      \n        Model extraction from counterfactual explanations. \n        Ulrich Aïvodji, Alexandre Bolot, Sébastien Gambs. , 2020.\n      \n      \n        Learning fair rule lists. \n        Julien Ferry, Ulrich Aïvodji, Sébastien Gambs, Marie-José Huguet, Mohamed Siala. , 2019.\n      \n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "TISL",
    "section": "",
    "text": "Learning fair rule lists with FairCORELS\n\n\n\n\n\n\n\nPython\n\n\nInterpretability\n\n\nFairness\n\n\n\n\nUsing Python to learn fair rule lists with FairCORELS\n\n\n\n\n\n\nMar 29, 2021\n\n\nUlrich Aïvodji\n\n\n\n\n\n\n  \n\n\n\n\nFairwashing in machine learning\n\n\n\n\n\n\n\nExplainability\n\n\nFairness\n\n\nFairwashing\n\n\n\n\nSummary of a paper: Fairwashing – the risk of rationalization\n\n\n\n\n\n\nApr 24, 2019\n\n\nUlrich Aïvodji\n\n\n\n\n\n\nNo matching items"
  }
]